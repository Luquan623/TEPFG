from ts_forecasting_traffic.model.trainer import TrainerBase
from torch import nn
from torch.utils.tensorboard import SummaryWriter
from atp.model.torchmodel import TorchModel
import numpy as np
import torch
import os
from ts_forecasting_traffic.model.STAEformer_class.model.STAEformer import Model
from tqdm import tqdm
import copy

import matplotlib.pyplot as plt



class Exp_STAEformer_class(TorchModel):
    def __init__(self):
        super(Exp_STAEformer_class,self).__init__()
        ## pretarin
        # parser
        self.device = 'cuda'
        self.cuda = True
        self.resume_state = False
        # data
        self.in_steps = 12
        self.out_steps = 12
        self.train_ratio = 0.6
        self.val_ratio = 0.2
        self.test_ratio = 0.2
        self.time_of_day = True
        self.day_of_week = True


        # model
        self.num_nodes = 307
        self.steps_per_day = 288
        self.input_dim = 3
        self.output_dim = 1
        self.input_embedding_dim = 24
        self.tod_embedding_dim = 24
        self.dow_embedding_dim = 24
        self.spatial_embedding_dim = 0
        self.adaptive_embedding_dim = 80
        self.feed_forward_dim = 256
        self.num_heads = 4
        self.num_layers = 3
        self.dropout = 0.1


        # train
        self.lr = 0.001
        self.weight_decay = 0.0005
        self.milestones = [15, 30, 50]
        self.lr_decay_rate = 0.1
        self.batch_size = 8
        self.epochs = 2
        self.use_cl = False
        self.cl_step_size = 2500


        # other
        self.pattern = 'train_alone'
        self.extreme_max = 1.6
        self.extreme_min = 1.6
        self.scaler = None
        self.dataset_use = ['PEMS04']
        self.Model = 'STAEformer'
        self.weather = False
        self.input_base_dim = 1
        self.extreme_labeling = False
        self.his = 12
        self.pred = 1

        self.extreme_ratio = 0.1
        self.finetune_batch_size = 8
        self.extreme_sample_num = 368
        self.finetune_epochs = 10
        self.finetune_sample_num = 1000
        self.loss_type_extreme = 'weighted_huber'  # å¯é€‰: 'mse', 'mae', 'huber', 'weighted_huber'
        self.detect_sample_num = 0 #
        self.dataset = 'PEMS04'
        self.root_path = './ts_forecasting_traffic/data'
        self.checkpoint_path_pntrain_model = 'ts_forecasting_traffic/checkpoints/STAEformer/PEMS04/checkpoint.pth'
        self.checkpoint_path_standard_model = 'ts_forecasting_traffic/checkpoints/STAEformer/PEMS04_standard/checkpoint.pth'
        self.finetune_path = 'ts_forecasting_traffic/checkpoints/STAEformer/PEMS04/finetune_type/checkpoint.pth'
        self.label = False
        self.label_save = "result/PEMS04/label/predictions00.npy"
        self.best_threshold = 0.5
        self.val_loss_curve = []  # ç”¨äºè®°å½•æ¯ä¸ª epoch çš„éªŒè¯æŸå¤±
        self.use_GMM = False
        self.save_possibility = False
    def _build_model(self):
        model = Model(
            self
        ).float()
        return model

    def opt_one_batch(self, inputs, targets=None):   # è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼Œè¿”å›è¯¥æ‰¹æ¬¡çš„æŸå¤±
        """
        Parameters
        ----------
        batch: è¾“å…¥çš„ä¼˜åŒ–æ•°æ®

        Returns
        -------
        è¿”å›ä¸€ä¸ªè‡³å°‘åŒ…å«'loss' å…³é”®å­—çš„å­—å…¸ã€‚ lossçš„å€¼è¡¨ç¤ºå½“å‰batæ•°æ®ä¸‹ç®—å‡ºæ¥çš„æŸå¤±å€¼ã€‚
        """
        inputs, targets = inputs.squeeze(0).to(self.device), targets.squeeze(0).to(self.device) # squeeze(0) ç”¨äºå»æ‰ç¬¬ä¸€ç»´ï¼ˆé€šå¸¸æ˜¯batch sizeä¸º1æ—¶ï¼‰ï¼Œç¡®ä¿è¾“å…¥å’Œç›®æ ‡éƒ½æ˜¯äºŒç»´çš„
        targets = targets[..., -self.output_dim:].float()

        # # æ‹†è§£æ¯ä¸€åˆ—
        # x0 = inputs[..., 0:1]  # ç¬¬ 1 ä¸ªç‰¹å¾
        # x1 = inputs[..., 1:2]  # ç¬¬ 2 ä¸ªç‰¹å¾
        # x2 = inputs[..., 2:3]  # ç¬¬ 3 ä¸ªç‰¹å¾
        # x3 = inputs[..., 3:4]  # ç¬¬ 4 ä¸ªç‰¹å¾
        # # è°ƒæ¢é¡ºåºï¼šå°† x0 å’Œ x3 å¯¹è°ƒ
        # inputs = torch.cat([x3, x1, x2, x0], dim=-1)

        out = self.model(inputs)
        self.optimizer.zero_grad()



        loss_pred = self.loss(out, targets)

        loss = loss_pred
        loss.backward()
        self.optimizer.step()

        Loss_dict = {}
        Loss_dict['loss'] = float( loss.data.cpu().numpy())  # ä¸ç®¡æ•°æ®åœ¨gpuè¿˜æ˜¯cpuéƒ½ç»Ÿä¸€å­˜å…¥cpu
        return Loss_dict

    def train(self, train_dataloader, val_dataloader=None, test_dataloader=None, valid_func=None, cb_progress=lambda x: None):


        self.model = self._build_model()  # å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡
        # æŠŠæ¨¡å‹æ”¾åˆ°gpuæˆ–cpuä¸Š
        self.model.to(self.device)

        # è®¾ç½®ä¼˜åŒ–æ–¹æ³•åŠç›¸å…³å‚æ•°
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=self.lr,
            weight_decay= self.weight_decay,
            eps= 1e-8,
        ) # å®šä¹‰äº†ä¼˜åŒ–å™¨
        ratio = 10
        pos_weight = torch.tensor([ratio], device=self.device)
        self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        writer = SummaryWriter(self.tensorboard_path)
        trainer = TrainerBase(self.epochs, valid_on_train_set=True)

        trainer.train(self, train_dataloader, val_dataloader, test_dataloader, valid_func, writer)
        os.makedirs(os.path.dirname(self.checkpoint_path_pntrain_model), exist_ok=True)
        self.plot_val_metric_curve(ylabel= valid_func.__class__.__name__)
        torch.save(self.model.state_dict(), self.checkpoint_path_pntrain_model)
    def _predict(self, inputs, targets):  # è¿”å›æ¯ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
        # # æ‹†è§£æ¯ä¸€åˆ—
        # x0 = inputs[..., 0:1]  # ç¬¬ 1 ä¸ªç‰¹å¾
        # x1 = inputs[..., 1:2]  # ç¬¬ 2 ä¸ªç‰¹å¾
        # x2 = inputs[..., 2:3]  # ç¬¬ 3 ä¸ªç‰¹å¾
        # x3 = inputs[..., 3:4]  # ç¬¬ 4 ä¸ªç‰¹å¾
        # # è°ƒæ¢é¡ºåºï¼šå°† x0 å’Œ x3 å¯¹è°ƒ
        # inputs = torch.cat([x3, x1, x2, x0], dim=-1)

        output= self.model(inputs)
        probs = torch.sigmoid(output)
        if self.save_possibility:
            output = probs
        else:
            output  = ( probs > self.best_threshold).int()
        y_lbl =  targets[..., -self.output_dim:].float()
        return output.detach().cpu().numpy(),y_lbl.detach().cpu().numpy()

    def eval_data(self, dataloader, metric, inbatch=None) -> float:  # è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆå¯ä»¥åœ¨è®­ç»ƒé›†ï¼ŒéªŒè¯é›†æˆ–è€…æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼‰


        self.model.eval()
        Y = []
        Pred = []
        with torch.no_grad():
            for inputs, targets in  dataloader:
                inputs, targets = inputs.squeeze(0).to(self.device), targets[..., -self.output_dim:].squeeze(0).float().to(self.device)
                # # æ‹†è§£æ¯ä¸€åˆ—
                # x0 = inputs[..., 0:1]  # ç¬¬ 1 ä¸ªç‰¹å¾
                # x1 = inputs[..., 1:2]  # ç¬¬ 2 ä¸ªç‰¹å¾
                # x2 = inputs[..., 2:3]  # ç¬¬ 3 ä¸ªç‰¹å¾
                # x3 = inputs[..., 3:4]  # ç¬¬ 4 ä¸ªç‰¹å¾
                # # è°ƒæ¢é¡ºåºï¼šå°† x0 å’Œ x3 å¯¹è°ƒ
                # inputs = torch.cat([x3, x1, x2, x0], dim=-1)

                out = self.model(inputs)  # å°†è¾“å…¥ã€ç›®æ ‡å’Œé€‰æ‹©çš„æ•°æ®é›†ä¼ å…¥æ¨¡å‹ï¼Œè¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°è¾“å‡º outã€‚
                probs = torch.sigmoid(out)
                out = ( probs > self.best_threshold).int()
                Y.append(targets.detach().cpu().numpy()) # çœŸå®å€¼
                Pred.append(out.detach().cpu().numpy()) # é¢„æµ‹å€¼
        Y = np.concatenate(Y)
        Pred = np.concatenate(Pred)
        self.model.train()
        score = metric(Y, Pred)
        self.val_loss_curve.append(score)
        return score

    def predict(self, test_dataloader, cb_progress=lambda x: None):  # è¿”å›æ‰€æœ‰çš„é¢„æµ‹ç»“æœY,Yæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
        """
        Args:
    .        ds: TSForecastingDataset ç»“æ„æ•°æ®ï¼Œ åœ¨æ¨¡å‹è¿è¡Œç»“æŸæ—¶è¿è¡Œ
            Return: è®­ç»ƒç»“æŸåæ¨¡å‹åœ¨æµ‹è¯•é›†è¿è¡Œç»“æœ
        """
        # self.model = self.model.to(self.device)
        if self.pattern == 'test':
            standard_model_path = self.checkpoint_path_standard_model
            self.model = self._build_model()
            self.model.load_state_dict(
                torch.load(standard_model_path, map_location=self.device))  # åŠ è½½æ ‡å‡†çš„æ¨¡å‹æƒé‡åˆ°å½“å‰æ¨¡å‹ä¸­
            self.model = self.model.to(self.device)
        Y = [] # çœŸå®å€¼
        Pred = []
        self.model.eval()
        with torch.no_grad():
            for inputs, targets in test_dataloader:
                inputs, targets = inputs.squeeze(0).to(self.device), targets.squeeze(0).to(self.device)
                pred,targets = self._predict(inputs, targets)
                Y.append(targets)
                Pred.append(pred)
        # 1. æå–ç›®å½•éƒ¨åˆ†
        save_dir = os.path.dirname(self.label_save)
        # 2. è‡ªåŠ¨åˆ›å»ºä¸Šçº§ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        # åˆå¹¶ & squeeze
        pred_array = np.concatenate(Pred, axis=0)
        if pred_array.shape[-1] == 1:
            pred_array = np.squeeze(pred_array, axis=-1)
        # ä¿å­˜ä¸º .npy æ–‡ä»¶
        np.save(self.label_save, pred_array)
        # ä¿å­˜ä¸º .csv æ–‡ä»¶ï¼ˆè½¬æ¢ä¸º 2D å†ä¿å­˜ï¼‰
        csv_ready_array = pred_array.reshape(pred_array.shape[0], -1)
        csv_path = os.path.join(save_dir, "predictions00.csv")
        np.savetxt(csv_path, csv_ready_array, delimiter=",", fmt="%.4f")
        if not self.save_possibility:
            # 1. æ€»ç‚¹æ•°ï¼ˆæ‰€æœ‰å…ƒç´ æ•°é‡ï¼‰
            total_points = pred_array.size
            # 2. æå€¼ç‚¹æ•°ï¼ˆå€¼ä¸º1çš„æ•°é‡ï¼‰
            extreme_points = np.sum(pred_array == 1)
            # 3. æå€¼æ¯”ä¾‹ï¼ˆç™¾åˆ†æ¯”ï¼‰
            extreme_ratio = extreme_points / total_points
            print(f"æ€»é¢„æµ‹ç‚¹æ•°: {total_points}")
            print(f"é¢„æµ‹ä¸ºæå€¼ï¼ˆ1ï¼‰çš„ç‚¹æ•°: {extreme_points}")
            print(f"æå€¼æ¯”ä¾‹: {extreme_ratio:.4%}")
        return np.squeeze(np.concatenate(Pred)),np.squeeze(np.concatenate(Y))

    # # éå†å¤šä¸ªé˜ˆå€¼  éœ€è¦self.save_possibilityä¸ºTrue
    # def predict(self, test_dataloader, cb_progress=lambda x: None):
    #     if self.pattern == 'test':
    #         standard_model_path = self.checkpoint_path_standard_model
    #         self.model = self._build_model()
    #         self.model.load_state_dict(torch.load(standard_model_path, map_location=self.device))
    #         self.model = self.model.to(self.device)
    #
    #     Y = []
    #     Pred = []
    #     self.model.eval()
    #     with torch.no_grad():
    #         for inputs, targets in test_dataloader:
    #             inputs, targets = inputs.squeeze(0).to(self.device), targets.squeeze(0).to(self.device)
    #             pred, targets = self._predict(inputs, targets)
    #             Y.append(targets)
    #             Pred.append(pred)
    #
    #     save_dir = os.path.dirname(self.label_save)
    #     os.makedirs(save_dir, exist_ok=True)
    #
    #     pred_array = np.concatenate(Pred, axis=0)
    #     if pred_array.shape[-1] == 1:
    #         pred_array = np.squeeze(pred_array, axis=-1)
    #     y_array = np.concatenate(Y, axis=0)
    #     if y_array.shape[-1] == 1:
    #         y_array = np.squeeze(y_array, axis=-1)
    #
    #     # éå†å¤šä¸ªé˜ˆå€¼
    #     threshold_list = np.arange(0.1, 1.0, 0.1)
    #     for threshold in threshold_list:
    #         binary_pred = (pred_array >= threshold).astype(int)
    #         # ä¿å­˜ npy
    #         npy_path = os.path.join(save_dir, f"pred_threshold_{threshold:.2f}.npy")
    #         np.save(npy_path, binary_pred)
    #         # æ‰“å°ä¿¡æ¯
    #         total_points = binary_pred.size
    #         extreme_points = np.sum(binary_pred == 1)
    #         extreme_ratio = extreme_points / total_points
    #         print(f"[é˜ˆå€¼={threshold:.2f}] æå€¼ç‚¹æ•°: {extreme_points}, å æ¯”: {extreme_ratio:.2%}")
    #
    #     return np.squeeze(pred_array), np.squeeze(y_array)

    # def predict(self, test_dataloader, cb_progress=lambda x: None):
    #     """
    #     æ‰§è¡Œæ¨¡å‹é¢„æµ‹ + PR æ›²çº¿åˆ†æ + è‡ªåŠ¨æœ€ä¼˜é˜ˆå€¼é€‰æ‹©
    #     """
    #     if self.pattern == 'test':
    #         standard_model_path = self.checkpoint_path_standard_model
    #         self.model = self._build_model()
    #         self.model.load_state_dict(
    #             torch.load(standard_model_path, map_location=self.device))
    #         self.model = self.model.to(self.device)
    #
    #     self.model.eval()
    #
    #     Y = []  # ground truth
    #     Logits = []  # raw model output (no sigmoid)
    #
    #     with torch.no_grad():
    #         for inputs, targets in test_dataloader:
    #             inputs, targets = inputs.squeeze(0).to(self.device), targets.squeeze(0).to(self.device)
    #
    #             # # æ‹†è§£é¡ºåº
    #             # x0 = inputs[..., 0:1]
    #             # x1 = inputs[..., 1:2]
    #             # x2 = inputs[..., 2:3]
    #             # x3 = inputs[..., 3:4]
    #             # inputs = torch.cat([x3, x1, x2, x0], dim=-1)
    #
    #             logits = self.model(inputs)  # (B, T, N, 1) raw logits
    #             probs = torch.sigmoid(logits)  # (B, T, N, 1)  æ˜ å°„æˆæ¦‚ç‡
    #             Logits.append( probs.detach().cpu().numpy())
    #             Y.append(targets[..., -self.output_dim:].detach().cpu().numpy())
    #
    #     # === åˆå¹¶ logits å’Œæ ‡ç­¾ ===
    #     logits_array = np.concatenate(Logits, axis=0)  # (B, T, N, 1)
    #
    #     y_true = np.concatenate(Y, axis=0)  # (B, T, N, 1)
    #
    #     # å»æ‰å°¾éƒ¨ç»´åº¦ï¼ˆå¦‚æœä¸º 1ï¼‰
    #     if logits_array.shape[-1] == 1:
    #         logits_array = np.squeeze(logits_array, axis=-1)  # -> (B, T, N)
    #     if y_true.shape[-1] == 1:
    #         y_true = np.squeeze(y_true, axis=-1)  # -> (B, T, N)
    #
    #     # reshape æˆäºŒç»´ç»“æ„ (æ—¶é—´ Ã— èŠ‚ç‚¹)
    #     logits_2d = logits_array.reshape(-1, logits_array.shape[-1])  # (BÃ—T, N)
    #     y_true_2d = y_true.reshape(-1, y_true.shape[-1])  # (BÃ—T, N)
    #
    #     # === PR åˆ†æ ===ï¼ˆflatten åè¯„ä¼°ï¼‰
    #     logits_flat = logits_2d.flatten()
    #     y_true_flat = y_true_2d.flatten()
    #
    #     from sklearn.metrics import precision_recall_curve, classification_report
    #     import matplotlib.pyplot as plt
    #
    #     precision, recall, thresholds = precision_recall_curve(y_true_flat, logits_flat)
    #     f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    #     best_idx = np.argmax(f1_scores)
    #     best_threshold = thresholds[best_idx]
    #
    #     print(f"\nğŸ“ˆ [PR æ›²çº¿åˆ†æ]")
    #     print(f"æœ€ä½³é˜ˆå€¼: {best_threshold:.3f}")
    #     print(f"Precision: {precision[best_idx]:.4f}")
    #     print(f"Recall   : {recall[best_idx]:.4f}")
    #     print(f"F1 Score : {f1_scores[best_idx]:.4f}")
    #
    #     # å¯è§†åŒ–
    #     plt.figure()
    #     plt.plot(recall, precision, label='PR Curve')
    #     plt.scatter(recall[best_idx], precision[best_idx], color='red', label='Best Threshold')
    #     plt.xlabel("Recall")
    #     plt.ylabel("Precision")
    #     plt.title("Precision-Recall Curve")
    #     plt.legend()
    #     plt.grid(True)
    #     plt.tight_layout()
    #     plt.show()
    #
    #     # === åˆ†ç±»é¢„æµ‹ï¼ˆä¿ç•™äºŒç»´ç»“æ„ï¼‰
    #     y_pred_best_2d = (logits_2d > best_threshold).astype(int)  # (BÃ—T, N)
    #
    #     # === æ‰“å°è¯„ä¼°æŠ¥å‘Šï¼ˆç”¨ 1Dï¼‰
    #     print("\nğŸ“Š ä½¿ç”¨æœ€ä½³é˜ˆå€¼çš„åˆ†ç±»è¯„ä¼°:")
    #     print(classification_report(y_true_flat, y_pred_best_2d.flatten(), digits=4))
    #
    #     # === ä¿å­˜ä¸º .npy
    #     save_dir = os.path.dirname(self.label_save)
    #     os.makedirs(save_dir, exist_ok=True)
    #     np.save(self.label_save, y_pred_best_2d)  # ç›´æ¥ä¿å­˜äºŒç»´æ ‡ç­¾
    #
    #     # === æå€¼æ¯”ä¾‹ç»Ÿè®¡
    #     total_points = y_pred_best_2d.size
    #     extreme_points = np.sum(y_pred_best_2d == 1)
    #     extreme_ratio = extreme_points / total_points
    #     print(f"\nğŸ§¾ æ€»é¢„æµ‹ç‚¹æ•°: {total_points}")
    #     print(f"é¢„æµ‹ä¸ºæå€¼çš„ç‚¹æ•°: {extreme_points}")
    #     print(f"æå€¼æ¯”ä¾‹: {extreme_ratio:.4%}")
    #
    #     # === è¿”å›ç»“æ„æ¸…æ™°çš„ç»“æœ
    #     return y_pred_best_2d, y_true_2d


    def plot_val_metric_curve(self, ylabel="Validation Metric"):
        """
        ç»˜åˆ¶æ¯ä¸ª epoch çš„éªŒè¯é›†è¯„ä¼°æŒ‡æ ‡æ›²çº¿ã€‚
        """
        if  len(self.val_loss_curve) == 0:
            print("âš ï¸ æœªè®°å½•éªŒè¯é›†è¯„ä¼°æŒ‡æ ‡ï¼Œè¯·ç¡®è®¤æ˜¯å¦å·²å¯ç”¨è®°å½•ã€‚")
            return

        plt.figure(figsize=(8, 5))
        plt.plot(range(1, len(self.val_loss_curve) + 1), self.val_loss_curve, marker='o')
        plt.xlabel("Epoch")
        plt.ylabel(ylabel)
        plt.title(f"{ylabel} per Epoch")
        plt.grid(True)
        plt.tight_layout()
        plt.show()




