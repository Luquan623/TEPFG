from ts_forecasting_traffic.model.trainer import TrainerBase
from torch import nn
from torch.utils.tensorboard import SummaryWriter
from atp.model.torchmodel import TorchModel
import numpy as np
import torch
import os
from ts_forecasting_traffic.model.STAEformer_finetune.model.STAEformer import Model
from tqdm import tqdm
import copy




class Exp_STAEformer_finetune(TorchModel):
    def __init__(self):
        super(Exp_STAEformer_finetune,self).__init__()
        ## pretarin
        # parser
        self.device = 'cuda'
        self.cuda = True
        self.resume_state = False
        # data
        self.in_steps = 12
        self.out_steps = 12
        self.train_ratio = 0.6
        self.val_ratio = 0.2
        self.test_ratio = 0.2
        self.time_of_day = True
        self.day_of_week = True


        # model
        self.num_nodes = 307
        self.steps_per_day = 288
        self.input_dim = 3
        self.output_dim = 1
        self.input_embedding_dim = 24
        self.tod_embedding_dim = 24
        self.dow_embedding_dim = 24
        self.spatial_embedding_dim = 0
        self.adaptive_embedding_dim = 80
        self.feed_forward_dim = 256
        self.num_heads = 4
        self.num_layers = 3
        self.dropout = 0.1
        self.use_mixed_proj = True

        # train
        self.lr = 0.001
        self.weight_decay = 0.0005
        self.milestones = [15, 30, 50]
        self.lr_decay_rate = 0.1
        self.batch_size = 8
        self.epochs = 2
        self.use_cl = False
        self.cl_step_size = 2500


        # other
        self.pattern = 'train_alone'
        self.extreme_max = 1.6
        self.extreme_min = 1.6
        self.scaler = None
        self.dataset_use = ['PEMS04']
        self.Model = 'STAEformer'
        self.weather = False
        self.input_base_dim = 1
        self.extreme_labeling = False
        self.his = 12
        self.pred = 12

        self.extreme_ratio = 0.1
        self.finetune_batch_size = 8
        self.extreme_sample_num = 368
        self.finetune_epochs = 10
        self.finetune_sample_num = 1000
        self.loss_type_extreme = 'weighted_huber'  # å¯é€‰: 'mse', 'mae', 'huber', 'weighted_huber'
        self.detect_sample_num = 0 #
        self.dataset = 'PEMS04'
        self.root_path = './ts_forecasting_traffic/data'
        self.checkpoint_path_pntrain_model = 'ts_forecasting_traffic/checkpoints/STAEformer/PEMS04/checkpoint.pth'
        self.checkpoint_path_standard_model = 'ts_forecasting_traffic/checkpoints/STAEformer/PEMS04_standard/checkpoint.pth'
        self.checkpoint_path_standard_extreme_model = 'ts_forecasting_traffic/checkpoints/STAEformer/PEMS04_standard/checkpoint_0_100_1.6'
        self.finetune_path = 'ts_forecasting_traffic/checkpoints/STAEformer/PEMS04/finetune_type/checkpoint.pth'
        self.merge = False
        self.label = False
        self.use_GMM = False
        self.use_possibility= False
        self.normal_weight = 0
        self.extreme_weight = 1
        self.label_path = "result/PEMS04/label/predictions_rec.npy"

    def _build_model(self):
        model = Model(
            self
        ).float()
        return model

    def opt_one_batch(self, inputs, targets=None):   # è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼Œè¿”å›è¯¥æ‰¹æ¬¡çš„æŸå¤±
        """
        Parameters
        ----------
        batch: è¾“å…¥çš„ä¼˜åŒ–æ•°æ®

        Returns
        -------
        è¿”å›ä¸€ä¸ªè‡³å°‘åŒ…å«'loss' å…³é”®å­—çš„å­—å…¸ã€‚ lossçš„å€¼è¡¨ç¤ºå½“å‰batæ•°æ®ä¸‹ç®—å‡ºæ¥çš„æŸå¤±å€¼ã€‚
        """
        inputs, targets = inputs.squeeze(0).to(self.device), targets.squeeze(0).to(self.device) # squeeze(0) ç”¨äºå»æ‰ç¬¬ä¸€ç»´ï¼ˆé€šå¸¸æ˜¯batch sizeä¸º1æ—¶ï¼‰ï¼Œç¡®ä¿è¾“å…¥å’Œç›®æ ‡éƒ½æ˜¯äºŒç»´çš„
        targets = targets[..., :self.output_dim]
        out = self.model(inputs)
        self.optimizer.zero_grad()

        # é€†å½’ä¸€åŒ–
        out = self.scaler.inverse_transform(out)
        targets = self.scaler.inverse_transform(targets)
        loss_pred = self.loss(out, targets)

        loss = loss_pred
        loss.backward()
        self.optimizer.step()

        Loss_dict = {}
        Loss_dict['loss'] = float( loss.data.cpu().numpy())  # ä¸ç®¡æ•°æ®åœ¨gpuè¿˜æ˜¯cpuéƒ½ç»Ÿä¸€å­˜å…¥cpu
        return Loss_dict

    def train(self, train_dataloader, val_dataloader=None, test_dataloader=None, valid_func=None, cb_progress=lambda x: None):


        self.model = self._build_model()  # å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡
        # æŠŠæ¨¡å‹æ”¾åˆ°gpuæˆ–cpuä¸Š
        self.model.to(self.device)

        # è®¾ç½®ä¼˜åŒ–æ–¹æ³•åŠç›¸å…³å‚æ•°
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=self.lr,
            weight_decay= self.weight_decay,
            eps= 1e-8,
        ) # å®šä¹‰äº†ä¼˜åŒ–å™¨
        if self.pattern == 'extreme_train':
            self.loss = WeightedExtremeMSELoss(extreme_threshold=1.6, mean=self.scaler.mean, std=self.scaler.std,
                               normal_weight=self.normal_weight, extreme_weight=self.extreme_weight)
        else:
            self.loss = nn.HuberLoss() # åŸè®ºæ–‡
        #self.loss = nn.MSELoss()
        #self.loss = nn.L1Loss()
        # self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
        #     self.optimizer,
        #     milestones=self.milestones,
        #     gamma=self.lr_decay_rate,
        #     verbose=False,
        # ) # è®¾ç½® å­¦ä¹ ç‡è°ƒåº¦å™¨
        # # tensorboardè®¾ç½®
        writer = SummaryWriter(self.tensorboard_path)

        #self.standard_scaler = train_ds.scaler

        # æ„å»ºè®­ç»ƒå™¨ï¼Œtrainerï¼Œè‡ªåŠ¨æ ¹æ®è®­ç»ƒæ•°æ®ã€éªŒè¯æ•°æ®å’ŒéªŒè¯å‡½æ•°è¿›è¡ŒéªŒè¯ï¼Œå¹¶å°†ä¸­é—´è¿‡ç¨‹è®°å½•åˆ°writerä¸­
        # éœ€è¦æ³¨æ„çš„æ˜¯ï¼šå½“å‰æ¨¡å‹å¿…é¡»å®ç°saveï¼Œloadï¼Œopt_one_batchï¼Œ eval_data å‡½æ•°
        # trainer = TrainerBase(self.nepochs)
        # å¦‚æœéœ€è¦æ˜¾ç¤ºè®­ç»ƒé›†ä¸Šçš„éªŒè¯ç»“æœï¼Œåˆ™ç”¨å¦‚ä¸‹å‡½æ•°æ„å»ºtrainner
        trainer = TrainerBase(self.epochs, valid_on_train_set=True)
        trainer.train(self, train_dataloader, val_dataloader, test_dataloader, valid_func, writer)
        os.makedirs(os.path.dirname(self.checkpoint_path_pntrain_model), exist_ok=True)
        torch.save(self.model.state_dict(), self.checkpoint_path_pntrain_model)
    def _predict(self, inputs, targets):  # è¿”å›æ¯ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
        #åˆ¤æ–­æ•°æ®æ˜¯å¦ä¸ºæå€¼
        is_extreme = is_extreme_data(targets, lower_threshold=-self.extreme_min, upper_threshold=self.extreme_max,
                                     extreme_ratio=self.extreme_ratio)
        if self.pattern in ['train_whole','finetune','merge_test']: # è¿™äº›æ¨¡å‹ä¸‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨èåˆç­–ç•¥
            if self.merge:# èåˆæ­£å¸¸æ¨¡å‹å’Œæç«¯æ¨¡å‹çš„ç»“æœ
                # label = inputs[..., 3] # shape: # shape: [8, 12, 307] inputæå€¼åˆ†å¸ƒï¼ˆç”¨å†å²çª—å£çš„æå€¼åˆ†å¸ƒæ¥ä»£æ›¿é¢„æµ‹çª—å£çš„æå€¼åˆ†å¸ƒæ¥é€‰æ‹©æ¨¡å‹ï¼‰
                label = targets[..., 3]  # shape: # shape: [8, 12, 307] æ¦‚ç‡æ ‡ç­¾æˆ–0,1æ ‡ç­¾ï¼Œç”¨åˆ†ç±»å™¨çš„ç»“æœæ¥é€‰æ‹©é¢„æµ‹æ¨¡å‹
                label = label.unsqueeze(-1)  # shape: [8, 12, 307, 1]
                output_no = self.model(inputs) # æ­£å¸¸æ¨¡å‹å¾—åˆ°çš„é¢„æµ‹
                output_ex = self.model_extreme(inputs) # æå€¼æ¨¡å‹å¾—åˆ°çš„é¢„æµ‹
                if self.use_possibility: # ä½¿ç”¨æ¦‚ç‡åŠ æƒï¼Œç”¨åˆ†ç±»æ ‡ç­¾çš„æ¦‚ç‡ç›´æ¥åŠ æƒä¸¤æ¨¡å‹çš„ç»“æœ
                    output = label * output_ex + (1 - label) * output_no
                else: # ä½¿ç”¨ç¡¬åˆ‡æ¢ï¼Œlabel==1 çš„ä½ç½®å–æç«¯æ¨¡å‹è¾“å‡ºï¼Œå¦åˆ™å–æ­£å¸¸æ¨¡å‹è¾“å‡ºã€‚
                    output = torch.where(label == 1, output_ex, output_no)

                targets = targets[..., :self.output_dim] # åªä¿ç•™å›å½’ç›®æ ‡éœ€è¦çš„é€šé“
                output = self.scaler.inverse_transform(output)
                y_lbl = self.scaler.inverse_transform(targets)
                return output.detach().cpu().numpy(), y_lbl.detach().cpu().numpy(), is_extreme

                # ç”¨æ­£å¸¸æ¨¡å‹çš„æå€¼åˆ†å¸ƒæˆ–è€…æå€¼æ¨¡å‹çš„æå€¼åˆ†å¸ƒæ¥åŠ æƒèåˆä¸¤æ¨¡å‹
                # output_no = self.model(inputs)
                # output_ex = self.model_extreme(inputs)
                # # extreme_mask = (output_no > self.extreme_max) | (output_no < -self.extreme_min)#æ­£å¸¸æ¨¡å‹æå€¼åˆ†å¸ƒ
                # extreme_mask = (output_ex > self.extreme_max) | (output_ex < -self.extreme_min)#æå€¼æ¨¡å‹æå€¼åˆ†å¸ƒ
                # output =  torch.where(extreme_mask, output_ex, output_no)
                # output = self.scaler.inverse_transform(output)
                # targets=targets[...,: self.output_dim]
                # y_lbl = self.scaler.inverse_transform(targets)
                # return output.detach().cpu().numpy(), y_lbl.detach().cpu().numpy(), is_extreme
            else: # ä¸é‡‡ç”¨èåˆç­–ç•¥ï¼Œå¦‚æœåˆ¤å®šè¯¥ batch å±äºæç«¯æ ·æœ¬ï¼Œå°±ç”¨æç«¯æ¨¡å‹é¢„æµ‹ï¼Œå¦åˆ™ç”¨æ­£å¸¸æ¨¡å‹é¢„æµ‹ï¼›
                # if is_extreme == 1:
                #     output = self.model_extreme(inputs)
                # elif is_extreme == 0:
                #     output = self.model(inputs)

                # output = self.model_extreme(inputs) # åªç”¨å¾®è°ƒå‡ºçš„æå€¼æ¨¡å‹é¢„æµ‹

                output = self.model(inputs)  # åªç”¨æ­£å¸¸æ¨¡å‹é¢„æµ‹
                output = self.scaler.inverse_transform(output)
                targets = targets[..., :self.output_dim]
                y_lbl = self.scaler.inverse_transform(targets)
                return output.detach().cpu().numpy(),y_lbl.detach().cpu().numpy(), is_extreme

        elif self.pattern in ['train', 'train_alone','extreme_train','oversampling']: # è¿™äº›æ¨¡å¼ä¸‹åªæœ‰å•æ¨¡å‹ï¼Œç›´æ¥é¢„æµ‹
            output = self.model(inputs)  # å°†è¾“å…¥ã€ç›®æ ‡å’Œé€‰æ‹©çš„æ•°æ®é›†ä¼ å…¥æ¨¡å‹ï¼Œè¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°è¾“å‡º outã€‚
            targets = targets[..., :self.output_dim]

            output = self.scaler.inverse_transform(output)
            y_lbl = self.scaler.inverse_transform(targets)


            return output.detach().cpu().numpy(),y_lbl.detach().cpu().numpy(),is_extreme

    def eval_data(self, dataloader, metric, inbatch=None) -> float:  # è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆå¯ä»¥åœ¨è®­ç»ƒé›†ï¼ŒéªŒè¯é›†æˆ–è€…æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼‰


        self.model.eval()

        Y = []
        Pred = []
        with torch.no_grad():
            for inputs, targets in  dataloader:
                inputs, targets = inputs.squeeze(0).to(self.device), targets[..., :self.output_dim].squeeze(0).to(self.device)
                out = self.model(inputs)  # å°†è¾“å…¥ã€ç›®æ ‡å’Œé€‰æ‹©çš„æ•°æ®é›†ä¼ å…¥æ¨¡å‹ï¼Œè¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°è¾“å‡º outã€‚

                out = self.scaler.inverse_transform(out)
                targets = self.scaler.inverse_transform(targets)
                loss_pred = self.loss(out, targets)
                Y.append(targets.detach().cpu().numpy()) # çœŸå®å€¼
                Pred.append(out.detach().cpu().numpy()) # é¢„æµ‹å€¼
        Y = np.concatenate(Y)
        Pred = np.concatenate(Pred)
        self.model.train()
        return metric(Y, Pred)

    def predict(self, test_dataloader, cb_progress=lambda x: None):  # è¿”å›æ‰€æœ‰çš„é¢„æµ‹ç»“æœY,Yæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
        """
        Args:
    .        ds: TSForecastingDataset ç»“æ„æ•°æ®ï¼Œ åœ¨æ¨¡å‹è¿è¡Œç»“æŸæ—¶è¿è¡Œ
            Return: è®­ç»ƒç»“æŸåæ¨¡å‹åœ¨æµ‹è¯•é›†è¿è¡Œç»“æœ
        """
        Y = [] # çœŸå®å€¼ï¼ˆé€æ‰¹æ”¶é›†ï¼‰
        Pred = [] # é¢„æµ‹å€¼ï¼ˆé€æ‰¹æ”¶é›†ï¼‰
        e_num = 0  # ç»Ÿè®¡ï¼šæ ‡è®°ä¸ºâ€œæå€¼è·¯å¾„â€çš„æ‰¹æ¬¡æ•°
        n_num = 0  # ç»Ÿè®¡ï¼šæ ‡è®°ä¸ºâ€œæ­£å¸¸è·¯å¾„â€çš„æ‰¹æ¬¡æ•°
        # self.model = self._build_model()
        # self.model.load_state_dict(
        #     torch.load(self.checkpoint_path_standard_model, map_location=self.device))
        # self.model = self.model.to(self.device)
        if self.pattern == 'merge_test': # åŠ è½½æ­£å¸¸æ¨¡å‹å’Œæå€¼æ¨¡å‹
            # 1) åŠ è½½â€œæ ‡å‡†æ¨¡å‹â€ï¼ˆæ­£å¸¸æ¨¡å‹ï¼‰
            standard_model_path = self.checkpoint_path_standard_model
            self.model = self._build_model()
            self.model.load_state_dict(
                torch.load(standard_model_path, map_location=self.device))  # åŠ è½½æ ‡å‡†çš„æ¨¡å‹æƒé‡åˆ°å½“å‰æ¨¡å‹ä¸­
            self.model = self.model.to(self.device)
            # 2) åŠ è½½â€œæç«¯æ¨¡å‹â€ï¼ˆåœ¨æ ‡å‡†æ¨¡å‹ä¸Šå¾®è°ƒå¾—åˆ°ï¼‰
            standard_extreme_model_path = self.checkpoint_path_standard_extreme_model
            self.model_extreme = self._build_model()
            self.model_extreme.load_state_dict(
                torch.load(standard_extreme_model_path, map_location=self.device))  # åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡åˆ°å½“å‰æ¨¡å‹ä¸­
            self.model_extreme = self.model_extreme.to(self.device)


        if self.pattern == 'finetune': # æå€¼æ¨¡å‹å·²ç»å¾®è°ƒå¥½ï¼Œåªéœ€è¦åŠ è½½æ­£å¸¸æ¨¡å‹
            standard_model_path = self.checkpoint_path_standard_model
            self.model = self._build_model()
            self.model.load_state_dict(
                torch.load(standard_model_path, map_location=self.device))  # åŠ è½½æ ‡å‡†çš„æ¨¡å‹æƒé‡åˆ°å½“å‰æ¨¡å‹ä¸­
            self.model = self.model.to(self.device)
        if self.pattern in ['train_whole', 'finetune','merge_test']:
            # è‹¥å½“å‰æµç¨‹ä¼šç”¨åˆ°æç«¯æ¨¡å‹åšæ¨ç†ï¼Œåˆ™å°†å…¶åˆ‡åˆ° eval æ¨¡å¼ï¼ˆå…³é—­ Dropout/BN çš„è®­ç»ƒåˆ†æ”¯ï¼‰
            self.model_extreme.eval()
        # æ ‡å‡†æ¨¡å‹è®¾ç½®ä¸º eval æ¨¡å¼
        self.model.eval()

        with torch.no_grad(): # è¯„ä¼°é˜¶æ®µä¸éœ€è¦æ¢¯åº¦
            for inputs, targets in test_dataloader:
                # å–å‡ºä¸€ä¸ª batchï¼Œå¹¶æŠŠæœ€å‰é¢çš„ batch ç»´åº¦ï¼ˆè‹¥ä¸º1ï¼‰å»æ‰ï¼Œå†æ¬åˆ°æŒ‡å®šè®¾å¤‡
                inputs, targets = inputs.squeeze(0).to(self.device), targets.squeeze(0).to(self.device)
                pred,targets,flag = self._predict(inputs, targets)
                Y.append(targets)
                Pred.append(pred)
                if flag == 1:
                    e_num += 1
                elif flag == 0:
                    n_num += 1
            print(e_num,n_num)
        # cb_progress(1.0)

        pred_array = np.squeeze(np.concatenate(Pred))   # shape: (T, N)
        true_array = np.squeeze(np.concatenate(Y))      # shape: (T, N)

        #
        mean = self.scaler.mean  # shape: (N,)
        std = self.scaler.std    # shape: (N,)


        # === 2. æå€¼æ ‡ç­¾åŒ–ï¼ˆæ ¹æ® self.extreme_max å’Œ extreme_minï¼‰===
        upper = mean + self.extreme_max * std
        lower = mean - self.extreme_min * std

        pred_label = ((pred_array > upper) | (pred_array < lower)).astype(int)
        true_label = ((true_array > upper) | (true_array < lower)).astype(int)

        # === 3. å·®å¼‚ç»Ÿè®¡åˆ†æ ===
        assert pred_label.shape == true_label.shape, f"é¢„æµ‹ä¸æ ‡ç­¾ shape ä¸ä¸€è‡´ï¼š{pred_label.shape} vs {true_label.shape}"

        no = np.sum(true_label == 0)
        ex = np.sum(true_label == 1)
        total_diff = np.sum(pred_label != true_label)
        count_0_to_1 = np.sum((true_label == 0) & (pred_label == 1))
        count_1_to_0 = np.sum((true_label == 1) & (pred_label == 0))

        print(f"\nğŸ“Š æå€¼è¯†åˆ«åˆ†æï¼š")
        print(f"å®é™…æ­£å¸¸ç‚¹ï¼š{no}")
        print(f"å®é™…æå€¼ç‚¹ï¼š{ex}")
        print(f"æ€»ä¸åŒç‚¹æ•°ï¼š{total_diff}")
        print(f"åŸä¸º 0ï¼Œé¢„æµ‹ä¸º 1ï¼ˆæ–°å¢æå€¼ï¼‰ï¼š{count_0_to_1}")
        print(f"åŸä¸º 1ï¼Œé¢„æµ‹ä¸º 0ï¼ˆåˆ æ‰æå€¼ï¼‰ï¼š{count_1_to_0}")

        return pred_array, true_array

        # return np.squeeze(np.concatenate(Pred)),np.squeeze(np.concatenate(Y))

    def finetune(self,retrain_loader ):

        if self.pattern == 'finetune':  # æ„å»ºæç«¯æ¨¡å‹
            standard_model_path = self.checkpoint_path_standard_model
            self.model_extreme = self._build_model()
            # ä»æ ‡å‡†æ¨¡å‹ï¼ˆæ­£å¸¸æ¨¡å‹ï¼‰çš„æƒé‡åˆå§‹åŒ–æç«¯æ¨¡å‹
            self.model_extreme.load_state_dict(torch.load(standard_model_path, map_location=self.device)) # åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡åˆ°å½“å‰æ¨¡å‹ä¸­
            self.model_extreme = self.model_extreme.to(self.device)
        else:
            # å¦‚æœä¸æ˜¯ä¸“é—¨çš„ finetune æ¨¡å¼ï¼Œå°±ä»å½“å‰ self.model æ·±æ‹·è´ä¸€ä»½ä½œä¸ºæç«¯æ¨¡å‹
            self.model_extreme = copy.deepcopy(self.model)

        # ===== å†»ç»“ Embedding å±‚å‚æ•° =====
        if hasattr(self.model_extreme, "input_proj"):
            for param in self.model_extreme.input_proj.parameters():
                param.requires_grad = False
        if hasattr(self.model_extreme, "tod_embedding"):
            for param in self.model_extreme.tod_embedding.parameters():
                param.requires_grad = False
        if hasattr(self.model_extreme, "dow_embedding"):
            for param in self.model_extreme.dow_embedding.parameters():
                param.requires_grad = False
        if hasattr(self.model_extreme, "node_emb"):
            self.model_extreme.node_emb.requires_grad = False
        if hasattr(self.model_extreme, "adaptive_embedding"):
            self.model_extreme.adaptive_embedding.requires_grad = False

        # ===== åªä¼˜åŒ–éœ€è¦æ›´æ–°çš„å‚æ•° =====
        self.optimizer_extreme = torch.optim.Adam(
            filter(lambda p: p.requires_grad, self.model_extreme.parameters()),
            lr=1e-4,
            weight_decay=self.weight_decay,
            eps=1e-8,
        )
        # # ä¸ºæç«¯æ¨¡å‹æ„å»ºä¼˜åŒ–å™¨ï¼ˆAdamï¼‰
        # self.optimizer_extreme = torch.optim.Adam(
        #     self.model_extreme.parameters(),
        #     lr=1e-4,
        #     weight_decay=self.weight_decay,
        #     eps=1e-8,
        # )  # å®šä¹‰äº†ä¼˜åŒ–å™¨  1e-5
        # æ ¹æ®é…ç½®é€‰æ‹©æç«¯æ¨¡å‹çš„æŸå¤±å‡½æ•°
        if self.loss_type_extreme == 'mse':
            self.loss_extreme = nn.MSELoss()
        elif self.loss_type_extreme == 'mae':
            self.loss_extreme = nn.L1Loss()
        elif self.loss_type_extreme == 'huber':
            self.loss_extreme = nn.HuberLoss()
        elif self.loss_type_extreme == 'weighted_huber':
            # è‡ªå®šä¹‰çš„åŠ æƒ Huberï¼Œé€šå¸¸å¯¹æç«¯æ ·æœ¬åŠ æ›´å¤§æƒé‡
            self.loss_extreme = WeightedHuberLoss(delta=1.0, weight_extreme=10.0)
        elif self.loss_type_extreme == 'extreme':
            # è‡ªå®šä¹‰çš„â€œæå€¼åŠ æƒ MSEâ€ï¼Œç”¨é˜ˆå€¼ä¸ (mean,std) åˆ¤å®šæç«¯ä¸å¦å¹¶åŠ æƒ
            self.loss_extreme = WeightedExtremeMSELoss(extreme_threshold=1.6, mean=self.scaler.mean, std=self.scaler.std,normal_weight=self.normal_weight,extreme_weight=self.extreme_weight)
        else:
            raise ValueError(f"Unknown loss_type_extreme: {self.loss_type_extreme}")
        # è¿›å…¥è®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ Dropout/BN çš„è®­ç»ƒåˆ†æ”¯ï¼‰
        self.model_extreme.train()
        for epoch in range(self.finetune_epochs):
            # ç”¨ tqdm åŒ…è£…æ•°æ®è¿­ä»£å™¨ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
            iter_data = (
                tqdm(
                    retrain_loader,
                    total=len(retrain_loader),
                    ncols=100,
                    desc=f"finetune {epoch}:" ))
            total_loss = 0.0
            num_batches = 0
            for i, (batch_x, batch_y) in enumerate(iter_data):
                # æ¢¯åº¦æ¸…é›¶
                self.optimizer_extreme.zero_grad()
                pred, true = self._process_one_batch_model(batch_x, batch_y)
                # åæ ‡å‡†åŒ–ï¼šæŠŠæ ‡å‡†åŒ–ç©ºé—´çš„é¢„æµ‹/çœŸå€¼è¿˜åŸåˆ°åŸå§‹é‡çº²
                pred = self.scaler.inverse_transform(pred)
                true = self.scaler.inverse_transform(true)
                # è®¡ç®—æŸå¤±ï¼ˆæ³¨æ„ï¼šè¿™é‡Œåœ¨â€œåŸå§‹é‡çº²â€ä¸Šè®¡ç®—ï¼‰
                curr_loss = self.loss_extreme(pred.to(self.device), true)   # è¿™é‡Œè¦ä¿®æ”¹
                total_loss += curr_loss.item()
                num_batches += 1
                curr_loss.backward()
                self.optimizer_extreme.step()
            # æ¯ä¸ª epoch æ‰“å°å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
            print(f"Epoch {epoch}: Average Loss = {avg_loss:.6f}")


        # torch.save(self.optimizer_extreme.state_dict(),self.finetune_path )

    def _process_one_batch_model(self, batch_x, batch_y):
        x = batch_x.float().to(self.device)
        batch_y = batch_y.float()
        outputs = self.model_extreme(x)

        true = batch_y[:, :, :, 0:1].to(self.device)  # æå–é¢„æµ‹çš„çœŸå®å€¼

        return outputs, true

def is_extreme_data(data, lower_threshold=-1.6, upper_threshold=1.6, extreme_ratio=0.2):
    """
    åˆ¤æ–­è¾“å…¥æ•°æ®æ˜¯å¦ä¸ºæç«¯æ•°æ®ã€‚

    å‚æ•°:
        data: torch.Tensor, shape=(batch_size, seq_len, num_nodes, 3)
              æ•°æ®æœ€åä¸€ç»´åˆ†åˆ«ä»£è¡¨äº¤é€šæµé‡ã€æ—¥ç‰¹å¾ã€å‘¨ç‰¹å¾ï¼Œæ•°æ®å·²æ ‡å‡†åŒ–
        lower_threshold: float, ä¸‹é˜ˆå€¼ï¼Œé»˜è®¤å€¼ä¸º -1.6
                         ä½äº lower_threshold çš„ç‚¹è§†ä¸ºæç«¯å€¼
        upper_threshold: float, ä¸Šé˜ˆå€¼ï¼Œé»˜è®¤å€¼ä¸º 1.6
                         é«˜äº upper_threshold çš„ç‚¹è§†ä¸ºæç«¯å€¼
        extreme_ratio: float, æç«¯ç‚¹æ•°é‡å æ¯”çš„é˜ˆå€¼ï¼Œé»˜è®¤æ˜¯ 10%
                       è¶…è¿‡è¯¥æ¯”ä¾‹çš„ç‚¹æ•°è®¤ä¸ºæ˜¯æç«¯æ•°æ®

    è¿”å›:
        int: è‹¥äº¤é€šæµé‡ä¸­æç«¯ç‚¹ï¼ˆè¶…è¿‡ä¸Šä¸‹é˜ˆå€¼ï¼‰çš„æ•°é‡è¶…è¿‡æ€»ç‚¹æ•°çš„10%ï¼Œè¿”å›1ï¼ˆæç«¯æ•°æ®ï¼‰ï¼Œå¦åˆ™è¿”å›0ï¼ˆæ­£å¸¸æ•°æ®ï¼‰
    """
    # æå–äº¤é€šæµé‡æ•°æ®ï¼ˆæ ‡å‡†åŒ–åçš„æ•°æ®ï¼Œå‡è®¾åœ¨æœ€åä¸€ç»´çš„ç¬¬0ä¸ªä½ç½®ï¼‰
    traffic_flow = data[..., 0]

    # åˆ¤æ–­å“ªäº›ç‚¹æ˜¯æç«¯å€¼ï¼ˆè¶…å‡ºä¸Šä¸‹é˜ˆå€¼èŒƒå›´çš„ç‚¹ï¼‰
    extreme_mask = (traffic_flow < lower_threshold) | (traffic_flow > upper_threshold)

    # ç»Ÿè®¡æç«¯å€¼ç‚¹çš„æ•°é‡
    extreme_count = torch.sum(extreme_mask)

    # è®¡ç®—æ€»ç‚¹æ•°
    total_points = traffic_flow.numel()

    # åˆ¤æ–­æç«¯å€¼ç‚¹çš„æ¯”ä¾‹æ˜¯å¦è¶…è¿‡é˜ˆå€¼ï¼ˆ10% é»˜è®¤ï¼‰
    return 1 if extreme_count >= extreme_ratio * total_points else 0
class WeightedHuberLoss(nn.Module):
    def __init__(self, delta=1.0, weight_extreme=5.0):
        super().__init__()
        self.delta = delta
        self.weight_extreme = weight_extreme

    def forward(self, pred, target):
        error = pred - target
        abs_error = torch.abs(error)
        quadratic = torch.minimum(abs_error, torch.tensor(self.delta).to(error.device))
        linear = abs_error - quadratic
        loss = 0.5 * quadratic ** 2 + self.delta * linear

        # å¯¹æå€¼ç‚¹åŠ æƒï¼ˆå‡è®¾æå€¼èŒƒå›´å·²æ ‡å‡†åŒ–åˆ° Â±1.6 ä»¥å¤–ï¼‰
        weight_mask = ((target > 1.6) | (target < -1.6)).float() * self.weight_extreme + 1
        loss = loss * weight_mask
        return loss.mean()

import torch
import torch.nn as nn

class WeightedExtremeMSELoss(nn.Module):
    def __init__(self, extreme_threshold=1.6, mean=0, std=0, normal_weight=1.0, extreme_weight=5.0):
        """
        :param extreme_threshold: æå€¼åˆ¤å®š z-score é˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰
        :param mean: åŸå§‹æ•°æ®å‡å€¼ï¼ˆéæ ‡å‡†åŒ–ï¼‰
        :param std: åŸå§‹æ•°æ®æ ‡å‡†å·®
        :param normal_weight: æ­£å¸¸å€¼æŸå¤±æƒé‡ï¼ˆå»ºè®®è®¾ç½®ä¸º 1.0ï¼‰
        :param extreme_weight: æå€¼æŸå¤±æƒé‡ï¼ˆå»ºè®®è®¾ç½®ä¸º >1.0ï¼Œå¦‚ 5.0ï¼‰
        """
        super().__init__()
        # ä¿å­˜å‡å€¼ã€æ ‡å‡†å·®ï¼ˆç”¨äºæ¢å¤åŸå§‹ç©ºé—´é˜ˆå€¼ï¼‰
        self.mean = mean
        self.std = std
        self.extreme_threshold = extreme_threshold
        # æ­£å¸¸æ ·æœ¬æƒé‡ä¸æå€¼æ ·æœ¬æƒé‡
        self.normal_weight = normal_weight
        self.extreme_weight = extreme_weight

    def forward(self, pred, target):
        """
        :param pred: æ¨¡å‹é¢„æµ‹å€¼ (B, T, N, 1)
        :param target: çœŸå®å€¼ (B, T, N, 1)
        :return: åŠ æƒ MSE
        """
        # æ™®é€šçš„ MSE è¯¯å·®
        error = pred - target
        loss = error ** 2

        # åœ¨åŸå§‹æ•°å€¼ç©ºé—´è®¡ç®—æå€¼é˜ˆå€¼
        lower_bound = self.mean - self.extreme_threshold * self.std
        upper_bound = self.mean + self.extreme_threshold * self.std

        # æ„é€ æå€¼æ©ç ï¼štarget è¶…è¿‡é˜ˆå€¼å°±è®°ä¸ºæå€¼ï¼ˆ=1.0ï¼‰ï¼Œå¦åˆ™ä¸ºæ­£å¸¸ï¼ˆ=0.0ï¼‰
        is_extreme = ((target > upper_bound) | (target < lower_bound)).float()
        # æƒé‡çŸ©é˜µï¼šæå€¼ç‚¹ç”¨ extreme_weightï¼Œæ­£å¸¸ç‚¹ç”¨ normal_weight
        weights = self.normal_weight * (1.0 - is_extreme) + self.extreme_weight * is_extreme
        weighted_loss = loss * weights
        return weighted_loss.mean()

        # # åˆ†å¼€æ±‚mean
        # # åˆ†åˆ«è®¡ç®—æå€¼å’Œæ­£å¸¸å€¼æŸå¤±
        # extreme_loss = (loss * is_extreme).sum()
        # normal_loss = (loss * (1 - is_extreme)).sum()
        # # åˆ†åˆ«è®¡ç®—æ ·æœ¬æ•°ï¼ˆé¿å…å‡å€¼é™¤ä»¥0ï¼‰
        # extreme_count = is_extreme.sum().clamp(min=1.0)
        # normal_count = (1 - is_extreme).sum().clamp(min=1.0)
        # # åŠ æƒç»„åˆä¸¤ä¸ªéƒ¨åˆ†çš„å¹³å‡æŸå¤±
        # extreme_loss_mean = extreme_loss / extreme_count
        # normal_loss_mean = normal_loss / normal_count
        # # ä½¿ç”¨æƒé‡è¿›è¡ŒåŠ æƒ
        # return self.extreme_weight * extreme_loss_mean + self.normal_weight * normal_loss_mean


